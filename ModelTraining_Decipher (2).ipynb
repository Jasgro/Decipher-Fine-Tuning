{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e94L_LPKwZMg"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUqZ0gHJwZMg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.55.4\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmozlWWwwZMg"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
        "    \"unsloth/codellama-34b-bnb-4bit\",\n",
        "    \"unsloth/tinyllama-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
        "    \"unsloth/gemma-2b-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "    # tokenizer_name = model_name, # Explicitly specify tokenizer name\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XBBEnI5P8Ix7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37d6d46e"
      },
      "source": [
        "### Loading and running inference with saved LoRA adapters\n",
        "\n",
        "To load your saved LoRA adapters for inference, you'll need to specify the path to where you saved them in Google Drive. Make sure the path is correct.\n",
        "\n",
        "Then, you can use the `FastLanguageModel.from_pretrained` function to load the model with the LoRA adapters. Finally, you can run inference using the loaded model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `ChatML` format for conversation style finetunes. We use [Open Assistant conversations](https://huggingface.co/datasets/philschmid/guanaco-sharegpt-style) in ShareGPT style. ChatML renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<|im_start|>system\n",
        "You are a helpful assistant.<|im_end|>\n",
        "<|im_start|>user\n",
        "What's the capital of France?<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Paris.\n",
        "```\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old` and our own optimized `unsloth` template.\n",
        "\n",
        "Normally one has to train `<|im_start|>` and `<|im_end|>`. We instead map `<|im_end|>` to be the EOS token, and leave `<|im_start|>` as is. This requires no additional training of additional tokens.\n",
        "\n",
        "Note ShareGPT uses `{\"from\": \"human\", \"value\" : \"Hi\"}` and not `{\"role\": \"user\", \"content\" : \"Hi\"}`, so we use `mapping` to map it.\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# After mounting, you will need to update the file path in the next cell\n",
        "# to point to your file in Google Drive, e.g., '/content/drive/My Drive/Your Folder/conversation_training_data.json'\n",
        "\n",
        "# You can list files in your Drive to find the correct path\n",
        "# !ls \"/content/drive/My Drive/\""
      ],
      "metadata": {
        "id": "0Rb7spfYO7FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7hLXBdhqagFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import load_dataset\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "#from datasets import load_dataset\n",
        "#dataset = load_dataset(\"philschmid/guanaco-sharegpt-style\", split = \"train\")\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/drive/My Drive/Colab Notebooks/content/Training Data/conversation_training_data.json\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHiVoToneynS"
      },
      "source": [
        "Let's see how the `ChatML` format works by printing the 5th element"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GSuKSSbpYKq"
      },
      "outputs": [],
      "source": [
        "dataset[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5iEWrUkevpE"
      },
      "outputs": [],
      "source": [
        "print(dataset[5][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuKOAUDpUeDL"
      },
      "source": [
        "If you're looking to make your own chat template, that also is possible! You must use the Jinja templating regime. We provide our own stripped down version of the `Unsloth template` which we find to be more efficient, and leverages ChatML, Zephyr and Alpaca styles.\n",
        "\n",
        "More info on chat templates on [our wiki page!](https://github.com/unslothai/unsloth/wiki#chat-templates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p31Z-S6FUieB"
      },
      "outputs": [],
      "source": [
        "unsloth_template = \\\n",
        "    \"{{ bos_token }}\"\\\n",
        "    \"{{ 'You are a helpful assistant to the user\\n' }}\"\\\n",
        "    \"{% for message in messages %}\"\\\n",
        "        \"{% if message['role'] == 'user' %}\"\\\n",
        "            \"{{ '>>> User: ' + message['content'] + '\\n' }}\"\\\n",
        "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
        "            \"{{ '>>> Assistant: ' + message['content'] + eos_token + '\\n' }}\"\\\n",
        "        \"{% endif %}\"\\\n",
        "    \"{% endfor %}\"\\\n",
        "    \"{% if add_generation_prompt %}\"\\\n",
        "        \"{{ '>>> Assistant: ' }}\"\\\n",
        "    \"{% endif %}\"\n",
        "unsloth_eos_token = \"eos_token\"\n",
        "\n",
        "if False:\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template = (unsloth_template, unsloth_eos_token,), # You must provide a template and EOS token\n",
        "        mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        "        map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! Since we're using `ChatML`, use `apply_chat_template` with `add_generation_prompt` set to `True` for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"[q6] Please rank the below criteria in order of importance (where 1 = most important and 6 = least important) when deciding on an ADAS ECU Hardware supplier\\nClick or drag each item into a rank position.\\nRow:\\n[r1] Product technical performance (e.g., best specs and feastures, high fail safety)\\n[r2] Cost competitiveness (e.g., low sales price)\\n[r3] Quality (e.g., high reliability, little changes during development)\\n[r4] Launch (e.g., execution of planned launch schedule)\\n[r5] Engineering capability (e.g., highly skilled engineers, good cooperation with OEM)\\n[r6] Strength of Product Roadmap (e.g., frequent introduction of new features)\\n[r7] ${q8.r1.open}\\n[r8] ${q8.r2.open}\\n[r9] ${q8.r3.open}\\n[r10] ${q8.r4.open}\\n[r11] ${q8.r5.open}\\nChoice:\\n[ch1] #1\\n[ch2] #2\\n[ch3] #3\\n[ch4] #4\\n[ch5] #5\\n[ch6] #6\\n[ch7] #7\\n[ch8] #8\\n[ch9] #9\\n[ch10] #10\\n[ch11] #11\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"[q6] Please rank the below criteria in order of importance (where 1 = most important and 6 = least important) when deciding on an ADAS ECU Hardware supplier\\nClick or drag each item into a rank position. 1. Product technical performance (e.g., best specs and feastures, high fail safety) 2. Cost competitiveness (e.g., low sales price) 3. Quality (e.g., high reliability, little changes during development) 4. Launch (e.g., execution of planned launch schedule)\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 512, use_cache = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"Please enter your company's apprxoximate annual revenue in USD.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 512, use_cache = True)"
      ],
      "metadata": {
        "id": "gOKNrijxpkr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_mistral-7b-instruct-v0.3-bnb-4bit\")  # Local saving\n",
        "tokenizer.save_pretrained(\"tokenizer_mistral-7b-instruct-v0.3-bnb-4bit\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78917ebe"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_tokenizer_dir = \"tokenizer_mistral-7b-instruct-v0.3-bnb-4bit\"\n",
        "destination_tokenizer_dir = \"/content/drive/My Drive/Colab Notebooks/lora_mistral-7b-instruct-v0.3-bnb-4bit_saved/tokenizer_mistral-7b-instruct-v0.3-bnb-4bit\" # Saving inside the same main folder\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "if not os.path.exists(destination_tokenizer_dir):\n",
        "    os.makedirs(destination_tokenizer_dir)\n",
        "\n",
        "# Copy the contents of the source directory to the destination directory\n",
        "shutil.copytree(source_tokenizer_dir, destination_tokenizer_dir, dirs_exist_ok=True)\n",
        "\n",
        "print(f\"Tokenizer files copied from {source_tokenizer_dir} to {destination_tokenizer_dir}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d78ef01"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_dir = \"lora_mistral-7b-instruct-v0.3-bnb-4bit\" # The folder where the model and tokenizer were saved\n",
        "destination_dir = \"/content/drive/My Drive/Colab Notebooks/lora_mistral-7b-instruct-v0.3-bnb-4bit_saved\" # Your desired path in Google Drive\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "if not os.path.exists(destination_dir):\n",
        "    os.makedirs(destination_dir)\n",
        "\n",
        "# Copy the contents of the source directory to the destination directory\n",
        "shutil.copytree(source_dir, destination_dir, dirs_exist_ok=True)\n",
        "\n",
        "print(f\"Files copied from {source_dir} to {destination_dir}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive/\"Colab Notebooks\""
      ],
      "metadata": {
        "id": "rnN3kvZb9C71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HJaBgcC19NSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if True: # Set to True to enable this section\n",
        "    from unsloth import FastLanguageModel\n",
        "    from unsloth.chat_templates import get_chat_template # Import get_chat_template\n",
        "    from transformers import TextStreamer # Import TextStreamer\n",
        "\n",
        "    # Specify the path to your saved LoRA model in Google Drive\n",
        "    # Make sure this path is correct\n",
        "    lora_model_path = \"/content/drive/My Drive/Colab Notebooks/lora_mistral-7b-instruct-v0.3-bnb-4bit_saved\" # Replace with your actual path\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = lora_model_path, # Load from your saved path\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "    # Explicitly re-apply the chat template after loading\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "        mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        "        map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        "    )\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"What is a famous tall tower in Paris?\"},\n",
        "    {\"from\": \"gpt\", \"value\": \"\"}, # Added the empty assistant message back\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
      ],
      "metadata": {
        "id": "SNDDhR-w7IAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"What is a famous tall tower in Paris?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoModelForPeftCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "    model = AutoModelForPeftCausalLM.from_pretrained(\n",
        "        \"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit=load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2YiW4cDwZMl"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9989d5ad"
      },
      "source": [
        "# Finetuning Functions\n",
        "Implement the plan to finetune multiple models, save their LoRA adapters, load them for inference, and evaluate their performance, ensuring the notebook is well-organized and clean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04371b38"
      },
      "source": [
        "## Define a function for finetuning\n",
        "\n",
        "### Subtask:\n",
        "Create a function that encapsulates the finetuning process (loading the base model, applying LoRA, loading and formatting the dataset, and training).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive/\"Colab Notebooks\"/finetuned_lora_models"
      ],
      "metadata": {
        "id": "LTZ271O6c9VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49f0ab90"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to encapsulate the finetuning process as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3456b18a"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import load_dataset\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "import torch\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def finetune_model(\n",
        "    model_name,\n",
        "    dataset_path,\n",
        "    lora_r,\n",
        "    lora_target_modules,\n",
        "    lora_alpha,\n",
        "    lora_dropout,\n",
        "    lora_bias,\n",
        "    train_batch_size,\n",
        "    gradient_accumulation_steps,\n",
        "    warmup_steps,\n",
        "    max_steps,\n",
        "    learning_rate,\n",
        "    optim,\n",
        "    weight_decay,\n",
        "    lr_scheduler_type,\n",
        "    seed,\n",
        "    max_seq_length,\n",
        "    num_train_epochs=1, # Added num_train_epochs parameter\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    handle_protobuf_error=False # New parameter to handle protobuf error\n",
        "):\n",
        "    \"\"\"\n",
        "    Finetunes a language model with LoRA adapters using the provided parameters.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name of the base model to load.\n",
        "        dataset_path (str): The path to the training dataset (JSON format).\n",
        "        lora_r (int): LoRA attention dimension.\n",
        "        lora_target_modules (list): List of module names to apply LoRA to.\n",
        "        lora_alpha (int): The alpha parameter for LoRA.\n",
        "        lora_dropout (float): The dropout probability for LoRA.\n",
        "        lora_bias (str): The bias parameter for LoRA (\"none\" is optimized).\n",
        "        train_batch_size (int): Batch size per device for training.\n",
        "        gradient_accumulation_steps (int): Number of gradient accumulation steps.\n",
        "        warmup_steps (int): Number of warmup steps for the learning rate scheduler.\n",
        "        max_steps (int): The maximum number of training steps.\n",
        "        learning_rate (float): The learning rate for the optimizer.\n",
        "        optim (str): The optimizer to use.\n",
        "        weight_decay (float): The weight decay to apply.\n",
        "        lr_scheduler_type (str): The type of learning rate scheduler.\n",
        "        seed (int): The random seed.\n",
        "        max_seq_length (int): The maximum sequence length.\n",
        "        num_train_epochs (int, optional): The number of training epochs. Defaults to 1. # Documenting the new parameter\n",
        "        dtype (torch.dtype, optional): The dtype for the model. Defaults to None.\n",
        "        load_in_4bit (bool, optional): Whether to load the model in 4-bit quantization. Defaults to True.\n",
        "        handle_protobuf_error (bool, optional): If True, downgrades protobuf to a compatible version. Defaults to False.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the trained model and tokenizer.\n",
        "    \"\"\"\n",
        "    # Handle protobuf compatibility issue if requested\n",
        "    if handle_protobuf_error:\n",
        "        print(\"Downgrading protobuf to version 3.20.x to handle compatibility issue...\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"protobuf==3.20.3\"])\n",
        "            print(\"Protobuf downgraded successfully.\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error downgrading protobuf: {e}\")\n",
        "            print(\"Continuing without downgrading, may encounter protobuf error.\")\n",
        "\n",
        "    # Load the base model\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_name,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=dtype,\n",
        "        load_in_4bit=load_in_4bit,\n",
        "    )\n",
        "\n",
        "    # Apply LoRA adapters\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=lora_r,\n",
        "        target_modules=lora_target_modules,\n",
        "        lora_alpha=lora_alpha,\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=lora_bias,\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=seed,\n",
        "        use_rslora=False,\n",
        "        loftq_config=None,\n",
        "    )\n",
        "\n",
        "    # Load and format the dataset\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template=\"chatml\",\n",
        "        mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
        "        map_eos_token=True,\n",
        "    )\n",
        "\n",
        "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "\n",
        "    def formatting_prompts_func(examples):\n",
        "        convos = examples[\"conversations\"]\n",
        "        texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
        "        return {\"text\": texts}\n",
        "\n",
        "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "    # Initialize and train the trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        packing=False,\n",
        "        args=SFTConfig(\n",
        "            per_device_train_batch_size=train_batch_size,\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "            warmup_steps=warmup_steps,\n",
        "            max_steps=max_steps,\n",
        "            learning_rate=learning_rate,\n",
        "            optim=optim,\n",
        "            weight_decay=weight_decay,\n",
        "            lr_scheduler_type=lr_scheduler_type,\n",
        "            seed=seed,\n",
        "            output_dir=\"outputs\",\n",
        "            report_to=\"none\",\n",
        "            num_train_epochs=num_train_epochs, # Using the new parameter\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    return model, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ade7a9b"
      },
      "source": [
        "## Define a function for saving lora adapters\n",
        "\n",
        "### Subtask:\n",
        "Create a function to save the finetuned LoRA adapters and tokenizer to a specified Google Drive path.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b425f872"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to save the finetuned LoRA adapters and tokenizer to Google Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d93e29fd"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def save_lora_to_drive(model, tokenizer, drive_path):\n",
        "    \"\"\"\n",
        "    Saves the finetuned LoRA adapters and tokenizer to a specified Google Drive path.\n",
        "\n",
        "    Args:\n",
        "        model: The finetuned model with LoRA adapters.\n",
        "        tokenizer: The tokenizer.\n",
        "        drive_path (str): The desired path in Google Drive to save the model and tokenizer.\n",
        "    \"\"\"\n",
        "    local_model_dir = \"lora_finetuned_model\"\n",
        "    local_tokenizer_dir = \"finetuned_tokenizer\"\n",
        "\n",
        "    # Save locally first\n",
        "    model.save_pretrained(local_model_dir)\n",
        "    tokenizer.save_pretrained(local_tokenizer_dir)\n",
        "    print(f\"Model saved locally to {local_model_dir}\")\n",
        "    print(f\"Tokenizer saved locally to {local_tokenizer_dir}\")\n",
        "\n",
        "    # Create the destination directory in Drive if it doesn't exist\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "    # Copy model files\n",
        "    destination_model_dir = os.path.join(drive_path, local_model_dir)\n",
        "    os.makedirs(destination_model_dir, exist_ok=True)\n",
        "    shutil.copytree(local_model_dir, destination_model_dir, dirs_exist_ok=True)\n",
        "    print(f\"Model files copied from {local_model_dir} to {destination_model_dir}\")\n",
        "\n",
        "    # Copy tokenizer files\n",
        "    destination_tokenizer_dir = os.path.join(drive_path, local_tokenizer_dir)\n",
        "    os.makedirs(destination_tokenizer_dir, exist_ok=True)\n",
        "    shutil.copytree(local_tokenizer_dir, destination_tokenizer_dir, dirs_exist_ok=True)\n",
        "    print(f\"Tokenizer files copied from {local_tokenizer_dir} to {destination_tokenizer_dir}\")\n",
        "\n",
        "    # Clean up local files (optional)\n",
        "    # shutil.rmtree(local_model_dir)\n",
        "    # shutil.rmtree(local_tokenizer_dir)\n",
        "    # print(\"Cleaned up local files.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdfecfd1"
      },
      "source": [
        "## Define a function for loading lora adapters and running inference\n",
        "\n",
        "### Subtask:\n",
        "Create a function to load the saved LoRA adapters and tokenizer and run inference with a given prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a029cb1f"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `load_lora_and_infer` function as requested, incorporating all the specified steps for loading the model and tokenizer from a Google Drive path and running inference with a text streamer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd637425"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "\n",
        "def load_lora_and_infer(\n",
        "    drive_path,\n",
        "    max_seq_length,\n",
        "    dtype,\n",
        "    load_in_4bit,\n",
        "    messages,\n",
        "    max_new_tokens=128\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads a saved LoRA model and tokenizer from Google Drive and runs inference.\n",
        "\n",
        "    Args:\n",
        "        drive_path (str): The Google Drive path to the saved LoRA model and tokenizer.\n",
        "        max_seq_length (int): The maximum sequence length.\n",
        "        dtype (torch.dtype): The dtype for the model.\n",
        "        load_in_4bit (bool): Whether the model was loaded in 4-bit quantization.\n",
        "        messages (list): A list of message dictionaries in the format\n",
        "                         [{\"from\": \"role\", \"value\": \"content\"}, ...].\n",
        "        max_new_tokens (int, optional): The maximum number of new tokens to generate.\n",
        "                                        Defaults to 128.\n",
        "    \"\"\"\n",
        "    # Load the model from the provided Google Drive path\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = drive_path, # Load from your saved path\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "\n",
        "    # Enable native 2x faster inference\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    # Explicitly re-apply the chat template\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "        mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        "        map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        "    )\n",
        "\n",
        "    # Apply the chat template to the input messages\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True, # Must add for generation\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Initialize a TextStreamer\n",
        "    text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    # Generate the output using the TextStreamer\n",
        "    _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = max_new_tokens, use_cache = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a67f46f5"
      },
      "source": [
        "## Define a function for evaluation\n",
        "\n",
        "### Subtask:\n",
        "Create a function to evaluate the performance of a loaded model based on your criteria (cosine similarity, similarity to 'goal output'). This function will take the model, tokenizer, test data (including 'goal output'), and potentially hyperparameter information as input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42d686a9"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `evaluate_model` function to iterate through test data, generate responses, and compare them to the goal output using cosine similarity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ecf2529"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "def evaluate_model(model, tokenizer, test_data, hyperparameters=None):\n",
        "    \"\"\"\n",
        "    Evaluates the performance of a loaded model based on cosine similarity\n",
        "    between generated responses and goal outputs.\n",
        "\n",
        "    Args:\n",
        "        model: The loaded model for inference.\n",
        "        tokenizer: The tokenizer.\n",
        "        test_data (list): A list of dictionaries, where each dictionary contains\n",
        "                          'input_prompt' and 'goal_output' keys.\n",
        "        hyperparameters (dict, optional): Dictionary of hyperparameters used\n",
        "                                          for finetuning (not used in this function,\n",
        "                                          but kept for signature consistency).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing individual evaluation results and an overall summary.\n",
        "              Includes a list of dictionaries with 'input_prompt', 'generated_output',\n",
        "              'goal_output', and 'similarity_score' for each test case, and the\n",
        "              'average_similarity_score'.\n",
        "    \"\"\"\n",
        "    # Initialize Sentence Transformer model for cosine similarity\n",
        "    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    evaluation_results = []\n",
        "    total_similarity_score = 0\n",
        "\n",
        "    for test_case in test_data:\n",
        "        input_prompt = test_case['input_prompt']\n",
        "        goal_output = test_case['goal_output']\n",
        "\n",
        "        # Prepare messages for the chat template\n",
        "        messages = [\n",
        "            {\"from\": \"human\", \"value\": input_prompt},\n",
        "            {\"from\": \"gpt\", \"value\": \"\"}, # Add an empty assistant message for generation prompt\n",
        "        ]\n",
        "\n",
        "        # Apply chat template and tokenize\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        # Generate response\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=128, # Limit generated tokens for evaluation\n",
        "            use_cache=True,\n",
        "            pad_token_id=tokenizer.eos_token_id, # Set pad_token_id to eos_token_id\n",
        "            do_sample=True, # Enable sampling for more varied responses\n",
        "            top_k=50, # Sample from top 50 tokens\n",
        "            top_p=0.95, # Sample from top tokens with cumulative probability 0.95\n",
        "            temperature=0.7, # Control randomness\n",
        "        )\n",
        "\n",
        "        # Decode the generated output, excluding the input prompt part\n",
        "        generated_output = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        try:\n",
        "            embeddings = sentence_model.encode([generated_output, goal_output])\n",
        "            similarity_score = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating similarity for prompt: {input_prompt}. Error: {e}\")\n",
        "            similarity_score = 0 # Assign 0 similarity on error\n",
        "\n",
        "        evaluation_results.append({\n",
        "            'input_prompt': input_prompt,\n",
        "            'generated_output': generated_output,\n",
        "            'goal_output': goal_output,\n",
        "            'similarity_score': similarity_score\n",
        "        })\n",
        "        total_similarity_score += similarity_score\n",
        "\n",
        "    # Calculate average similarity score\n",
        "    average_similarity_score = total_similarity_score / len(test_data) if test_data else 0\n",
        "\n",
        "    return {\n",
        "        'individual_results': evaluation_results,\n",
        "        'average_similarity_score': average_similarity_score\n",
        "    }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14438df5"
      },
      "source": [
        "## Finetune and save models\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the list of models you want to finetune. For each model: Call the finetuning function with the appropriate model name and hyperparameters. Call the saving function to save the LoRA adapters and tokenizer to a unique path in Google Drive. Record the model name and hyperparameters used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eda6d26"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the list of models and hyperparameters, the base Google Drive path, and the list to store results, then loop through the models, construct the save path, call the finetune and save functions, and record the hyperparameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a0770ff"
      },
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# 1. Define a list of model names and corresponding hyperparameters\n",
        "model_configs = [\n",
        "    {\n",
        "        \"model_name\": \"unsloth/gemma-7b-bnb-4bit\",\n",
        "        \"lora_r\": 16,\n",
        "        \"lora_alpha\": 16,\n",
        "        \"lora_target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        \"lora_dropout\": 0,\n",
        "        \"lora_bias\": \"none\",\n",
        "        \"train_batch_size\": 2,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"warmup_steps\": 5,\n",
        "        \"max_steps\": 60, # Keeping max_steps small for demonstration\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"optim\": \"adamw_8bit\",\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"lr_scheduler_type\": \"linear\",\n",
        "        \"seed\": 3407,\n",
        "        \"num_train_epochs\": 1, # Added num_train_epochs to config\n",
        "    },\n",
        "    # Add more model configurations here if needed\n",
        "    # {\n",
        "    #     \"model_name\": \"unsloth/gemma-7b-bnb-4bit\",\n",
        "    #     \"lora_r\": 32,\n",
        "    #     \"lora_alpha\": 32,\n",
        "    #     \"lora_target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    #     \"lora_dropout\": 0.1,\n",
        "    #     \"lora_bias\": \"none\",\n",
        "    #     \"train_batch_size\": 4,\n",
        "    #     \"gradient_accumulation_steps\": 2,\n",
        "    #     \"warmup_steps\": 10,\n",
        "    #     \"max_steps\": 80,\n",
        "    #     \"learning_rate\": 1e-4,\n",
        "    #     \"optim\": \"adamw_8bit\",\n",
        "    #     \"weight_decay\": 0.05,\n",
        "    #     \"lr_scheduler_type\": \"cosine\",\n",
        "    #     \"seed\": 4242,\n",
        "    # }\n",
        "]\n",
        "\n",
        "# 2. Define the base Google Drive path\n",
        "base_drive_path = \"/content/drive/My Drive/Colab Notebooks/finetuned_lora_models\"\n",
        "dataset_path = \"/content/drive/My Drive/Colab Notebooks/content/Training Data/conversation_training_data.json\"\n",
        "\n",
        "# Ensure the base directory exists\n",
        "os.makedirs(base_drive_path, exist_ok=True)\n",
        "\n",
        "# 3. Initialize an empty list to store the model names and hyperparameters\n",
        "finetuning_results = []\n",
        "\n",
        "# Use existing global variables if they exist, otherwise define defaults\n",
        "if 'max_seq_length' not in globals():\n",
        "    max_seq_length = 2048\n",
        "if 'dtype' not in globals():\n",
        "    dtype = None\n",
        "if 'load_in_4bit' not in globals():\n",
        "    load_in_4bit = True\n",
        "\n",
        "# 4. Loop through the list of models\n",
        "for config in model_configs:\n",
        "    model_name = config[\"model_name\"]\n",
        "    lora_r = config[\"lora_r\"]\n",
        "    lora_alpha = config[\"lora_alpha\"]\n",
        "    lora_target_modules = config[\"lora_target_modules\"]\n",
        "    lora_dropout = config[\"lora_dropout\"]\n",
        "    lora_bias = config[\"lora_bias\"]\n",
        "    train_batch_size = config[\"train_batch_size\"]\n",
        "    gradient_accumulation_steps = config[\"gradient_accumulation_steps\"]\n",
        "    warmup_steps = config[\"warmup_steps\"]\n",
        "    max_steps = config[\"max_steps\"]\n",
        "    learning_rate = config[\"learning_rate\"]\n",
        "    optim = config[\"optim\"]\n",
        "    weight_decay = config[\"weight_decay\"]\n",
        "    lr_scheduler_type = config[\"lr_scheduler_type\"]\n",
        "    seed = config[\"seed\"]\n",
        "    num_train_epochs = config.get(\"num_train_epochs\", 1) # Get epochs with default to 1\n",
        "\n",
        "    print(f\"Starting finetuning for model: {model_name} with {num_train_epochs} epochs\")\n",
        "    print(f\"Hyperparameters: {config}\")\n",
        "\n",
        "    # 5. Inside the loop, for each model configuration:\n",
        "    # Construct a unique Google Drive path including epochs and timestamp\n",
        "    model_save_name = model_name.replace(\"/\", \"_\").replace(\"-\", \"_\") + f\"_epochs_{num_train_epochs}_{int(time.time())}\"\n",
        "    unique_drive_path = os.path.join(base_drive_path, model_save_name)\n",
        "\n",
        "    try:\n",
        "        # Call the finetune_model function with handle_protobuf_error=True and num_train_epochs\n",
        "        finetuned_model, finetuned_tokenizer = finetune_model(\n",
        "            model_name=model_name,\n",
        "            dataset_path=dataset_path,\n",
        "            lora_r=lora_r,\n",
        "            lora_target_modules=lora_target_modules,\n",
        "            lora_alpha=lora_alpha,\n",
        "            lora_dropout=lora_dropout,\n",
        "            lora_bias=lora_bias,\n",
        "            train_batch_size=train_batch_size,\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "            warmup_steps=warmup_steps,\n",
        "            max_steps=max_steps,\n",
        "            learning_rate=learning_rate,\n",
        "            optim=optim,\n",
        "            weight_decay=weight_decay,\n",
        "            lr_scheduler_type=lr_scheduler_type,\n",
        "            seed=seed,\n",
        "            max_seq_length=max_seq_length,\n",
        "            num_train_epochs=num_train_epochs, # Pass the number of epochs\n",
        "            dtype=dtype,\n",
        "            load_in_4bit=load_in_4bit,\n",
        "            handle_protobuf_error=True # Set to True to handle the protobuf error\n",
        "        )\n",
        "\n",
        "        # Call the save_lora_to_drive function\n",
        "        save_lora_to_drive(finetuned_model, finetuned_tokenizer, unique_drive_path)\n",
        "\n",
        "        # Record the model name and hyperparameters used\n",
        "        finetuning_results.append({\n",
        "            \"model_name\": model_name,\n",
        "            \"save_path\": unique_drive_path,\n",
        "            \"hyperparameters\": config\n",
        "        })\n",
        "        print(f\"Finetuning and saving completed for {model_name} with {num_train_epochs} epochs\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error finetuning or saving model {model_name} with {num_train_epochs} epochs: {e}\")\n",
        "        # Optionally, record the failure\n",
        "        finetuning_results.append({\n",
        "            \"model_name\": model_name,\n",
        "            \"save_path\": None,\n",
        "            \"hyperparameters\": config,\n",
        "            \"status\": \"failed\",\n",
        "            \"error\": str(e)\n",
        "        })\n",
        "\n",
        "\n",
        "# 6. After the loop finishes, print the recorded list\n",
        "print(\"\\n--- Finetuning Summary ---\")\n",
        "for result in finetuning_results:\n",
        "    print(f\"Model: {result['model_name']}\")\n",
        "    print(f\"Save Path: {result.get('save_path', 'N/A')}\")\n",
        "    print(f\"Hyperparameters: {result['hyperparameters']}\")\n",
        "    if 'status' in result and result['status'] == 'failed':\n",
        "        print(f\"Status: Failed - {result['error']}\")\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d2e1a19"
      },
      "source": [
        "## Load and evaluate models\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the saved LoRA models in your Google Drive. For each model: Call the loading and inference function to load the model and run inference on test examples. Call the evaluation function to assess the model's performance and save the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56f2f692"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the test data and initialize the evaluation results list, then iterate through the finetuning results, load each model, run inference, evaluate, and store the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75294679"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "\n",
        "# 1. Define test data\n",
        "test_data = [\n",
        "    {'input_prompt': '[q6] Please rank the below criteria in order of importance (where 1 = most important and 6 = least important) when deciding on an ADAS ECU Hardware supplier\\nClick or drag each item into a rank position.\\nRow:\\n[r1] Product technical performance (e.g., best specs and feastures, high fail safety)\\n[r2] Cost competitiveness (e.g., low sales price) 3. Quality (e.g., high reliability, little changes during development) 4. Launch (e.g., execution of planned launch schedule)',\n",
        "     'goal_output': '<select xmlns:builder=\"http://decipherinc.com/builder\" xmlns:ss=\"http://decipherinc.com/ss\" xmlns:html=\"http://decipherinc.com/html\" xmlns:autosum=\"http://decipherinc.com/autosum\" xmlns:slidernumber=\"http://decipherinc.com/slidernumber\" label=\"q6\" randomize=\"0\" shuffle=\"rows\" id=\"KQKQd\">\\n     <title id=\"QQQQa\">Please rank the below criteria in order of importance (where 1 = most important and 6 = least important) when deciding on an ADAS ECU Hardware supplier</title>\\n     <comment id=\"QQQQb\">Click or drag each item into a rank position.</comment>\\n     <row label=\"r1\" id=\"QQQQc\">Product technical performance (e.g., best specs and feastures, high fail safety)</row>\\n     <row label=\"r2\" id=\"QQQQd\">Cost competitiveness (e.g., low sales price)</row>\\n     <row label=\"r3\" id=\"QQQQe\">Quality (e.g., high reliability, little changes during development)</row>\\n     <row label=\"r4\" id=\"QQQQf\">Launch (e.g., execution of planned launch schedule)</row>\\n   </select>'},\n",
        "    {'input_prompt': 'Please enter your company\\'s apprxoximate annual revenue in USD.',\n",
        "     'goal_output': '<number label=\"Q10\" optional=\"0\" size=\"10\" id=\"V1Q1d\"> <title id=\"Q10_1\">What is your company\\'s apprximate annual revenue in USD?</title> <comment id=\"Q10_2\">Please enter a number</comment> </number>'},\n",
        "     {'input_prompt': 'What is a famous tall tower in Paris?',\n",
        "      'goal_output': '<radio label=\"Q10\" id=\"V1Q1k\"> <title id=\"Q1Q1a\">What is a famous tall tower in Paris?</title> <comment id=\"Q1Q1b\">Select one</comment> <row label=\"r1\" id=\"Q1Q1c\">Eiffel Tower</row> <row label=\"r2\" id=\"Q1Q1d\">Tour Montparnasse</row> <row label=\"r3\" id=\"Q1Q1e\">Tour Eiffel</row> </radio>'},\n",
        "]\n",
        "\n",
        "# 2. Initialize an empty list to store evaluation results\n",
        "all_evaluation_results = []\n",
        "\n",
        "# Use existing global variables if they exist, otherwise define defaults\n",
        "if 'max_seq_length' not in globals():\n",
        "    max_seq_length = 2048\n",
        "if 'dtype' not in globals():\n",
        "    dtype = None\n",
        "if 'load_in_4bit' not in globals():\n",
        "    load_in_4bit = True\n",
        "if 'finetuning_results' not in globals():\n",
        "    finetuning_results = [] # Initialize if not already defined\n",
        "\n",
        "# 3. Iterate through the finetuning_results list\n",
        "for result in finetuning_results:\n",
        "    model_name = result[\"model_name\"]\n",
        "    save_path = result[\"save_path\"]\n",
        "    hyperparameters = result[\"hyperparameters\"]\n",
        "\n",
        "    if save_path is None:\n",
        "        print(f\"Skipping evaluation for failed finetuning run: {model_name}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nEvaluating model: {model_name}\")\n",
        "    print(f\"Loading from path: {save_path}\")\n",
        "\n",
        "    try:\n",
        "        # Load the model and tokenizer\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name = save_path,\n",
        "            max_seq_length = max_seq_length,\n",
        "            dtype = dtype,\n",
        "            load_in_4bit = load_in_4bit,\n",
        "        )\n",
        "        FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "        # Explicitly re-apply the chat template\n",
        "        tokenizer = get_chat_template(\n",
        "            tokenizer,\n",
        "            chat_template = \"chatml\",\n",
        "            mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
        "            map_eos_token = True,\n",
        "        )\n",
        "\n",
        "        # 4.d. Call the evaluation function\n",
        "        evaluation_result = evaluate_model(model, tokenizer, test_data, hyperparameters)\n",
        "\n",
        "        # 4.e. Store the evaluation results\n",
        "        all_evaluation_results.append({\n",
        "            \"model_name\": model_name,\n",
        "            \"save_path\": save_path,\n",
        "            \"hyperparameters\": hyperparameters,\n",
        "            \"evaluation_results\": evaluation_result\n",
        "        })\n",
        "        print(f\"Evaluation completed for {model_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or evaluating model {model_name} from {save_path}: {e}\")\n",
        "        all_evaluation_results.append({\n",
        "            \"model_name\": model_name,\n",
        "            \"save_path\": save_path,\n",
        "            \"hyperparameters\": hyperparameters,\n",
        "            \"evaluation_results\": None,\n",
        "            \"status\": \"evaluation_failed\",\n",
        "            \"error\": str(e)\n",
        "        })\n",
        "\n",
        "\n",
        "# 5. Print or display the collected evaluation results\n",
        "print(\"\\n--- Evaluation Summary ---\")\n",
        "for result in all_evaluation_results:\n",
        "    print(f\"Model: {result['model_name']}\")\n",
        "    print(f\"Save Path: {result['save_path']}\")\n",
        "    print(f\"Hyperparameters: {result['hyperparameters']}\")\n",
        "    if result.get('status') == 'evaluation_failed':\n",
        "        print(f\"Status: Evaluation Failed - {result['error']}\")\n",
        "    elif result['evaluation_results']:\n",
        "        print(f\"Average Similarity Score: {result['evaluation_results']['average_similarity_score']:.4f}\")\n",
        "        # Optionally print individual results\n",
        "        # print(\"Individual Results:\")\n",
        "        # for ind_res in result['evaluation_results']['individual_results']:\n",
        "        #     print(f\"  Prompt: {ind_res['input_prompt'][:50]}...\")\n",
        "        #     print(f\"  Generated: {ind_res['generated_output'][:50]}...\")\n",
        "        #     print(f\"  Goal: {ind_res['goal_output'][:50]}...\")\n",
        "        #     print(f\"  Similarity: {ind_res['similarity_score']:.4f}\")\n",
        "    else:\n",
        "        print(\"Evaluation results not available.\")\n",
        "    print(\"-\" * 20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ac62b84"
      },
      "source": [
        "**Reasoning**:\n",
        "The evaluation failed because the required model files were not found in the specified save path. This indicates an issue with the saving process. The previous saving function saved the LoRA adapters and tokenizer into subdirectories within the specified drive path, but the loading function expects the model files directly in the provided path. I need to modify the loading and evaluation logic to correctly point to the saved adapter files within the saved model directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76696fd6"
      },
      "source": [
        "# 3. Iterate through the finetuning_results list\n",
        "all_evaluation_results = []\n",
        "\n",
        "for result in finetuning_results:\n",
        "    model_name = result[\"model_name\"]\n",
        "    save_path = result[\"save_path\"]\n",
        "    hyperparameters = result[\"hyperparameters\"]\n",
        "\n",
        "    if save_path is None or not os.path.exists(save_path):\n",
        "        print(f\"Skipping evaluation for failed or missing finetuning run: {model_name} at {save_path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nEvaluating model: {model_name}\")\n",
        "    print(f\"Loading from path: {save_path}\")\n",
        "\n",
        "    # Construct the correct path to the saved LoRA adapters\n",
        "    # Assuming the save_lora_to_drive function saves to a subdirectory named \"lora_finetuned_model\"\n",
        "    lora_adapter_path = os.path.join(save_path, \"lora_finetuned_model\")\n",
        "\n",
        "    if not os.path.exists(lora_adapter_path):\n",
        "        print(f\"LoRA adapter directory not found at {lora_adapter_path}. Skipping evaluation for {model_name}.\")\n",
        "        all_evaluation_results.append({\n",
        "            \"model_name\": model_name,\n",
        "            \"save_path\": save_path,\n",
        "            \"hyperparameters\": hyperparameters,\n",
        "            \"evaluation_results\": None,\n",
        "            \"status\": \"evaluation_failed\",\n",
        "            \"error\": f\"LoRA adapter directory not found at {lora_adapter_path}\"\n",
        "        })\n",
        "        continue\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Load the model and tokenizer using the path to the saved adapters\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name = lora_adapter_path, # Load from the saved adapter path\n",
        "            max_seq_length = max_seq_length,\n",
        "            dtype = dtype,\n",
        "            load_in_4bit = load_in_4bit,\n",
        "        )\n",
        "        FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "        # Explicitly re-apply the chat template\n",
        "        tokenizer = get_chat_template(\n",
        "            tokenizer,\n",
        "            chat_template = \"chatml\",\n",
        "            mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
        "            map_eos_token = True,\n",
        "        )\n",
        "\n",
        "        # Call the evaluation function\n",
        "        evaluation_result = evaluate_model(model, tokenizer, test_data, hyperparameters)\n",
        "\n",
        "        # Store the evaluation results\n",
        "        all_evaluation_results.append({\n",
        "            \"model_name\": model_name,\n",
        "            \"save_path\": save_path,\n",
        "            \"hyperparameters\": hyperparameters,\n",
        "            \"evaluation_results\": evaluation_result,\n",
        "            \"status\": \"success\"\n",
        "        })\n",
        "        print(f\"Evaluation completed for {model_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or evaluating model {model_name} from {lora_adapter_path}: {e}\")\n",
        "        all_evaluation_results.append({\n",
        "            \"model_name\": model_name,\n",
        "            \"save_path\": save_path,\n",
        "            \"hyperparameters\": hyperparameters,\n",
        "            \"evaluation_results\": None,\n",
        "            \"status\": \"evaluation_failed\",\n",
        "            \"error\": str(e)\n",
        "        })\n",
        "\n",
        "\n",
        "# 5. Print or display the collected evaluation results\n",
        "print(\"\\n--- Evaluation Summary ---\")\n",
        "for result in all_evaluation_results:\n",
        "    print(f\"Model: {result['model_name']}\")\n",
        "    print(f\"Save Path: {result['save_path']}\")\n",
        "    print(f\"Hyperparameters: {result['hyperparameters']}\")\n",
        "    if result.get('status') == 'evaluation_failed':\n",
        "        print(f\"Status: Evaluation Failed - {result['error']}\")\n",
        "    elif result.get('status') == 'success' and result['evaluation_results']:\n",
        "        print(f\"Status: Success\")\n",
        "        print(f\"Average Similarity Score: {result['evaluation_results']['average_similarity_score']:.4f}\")\n",
        "        # Optionally print individual results\n",
        "        # print(\"Individual Results:\")\n",
        "        # for ind_res in result['evaluation_results']['individual_results']:\n",
        "        #     print(f\"  Prompt: {ind_res['input_prompt'][:50]}...\")\n",
        "        #     print(f\"  Generated: {ind_res['generated_output'][:50]}...\")\n",
        "        #     print(f\"  Goal: {ind_res['goal_output'][:50]}...\")\n",
        "        #     print(f\"  Similarity: {ind_res['similarity_score']:.4f}\")\n",
        "    else:\n",
        "        print(\"Evaluation results not available.\")\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00da40da"
      },
      "source": [
        "## Compare evaluation results\n",
        "\n",
        "### Subtask:\n",
        "Analyze the saved evaluation results to compare the performance of the different models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aba3c8d"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the evaluation results, print the model name, hyperparameters, and average similarity score for each, identify the best performing model, and print a concluding statement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54327de6"
      },
      "source": [
        "best_model = None\n",
        "highest_avg_similarity = -1\n",
        "\n",
        "print(\"\\n--- Detailed Evaluation Results ---\")\n",
        "for result in all_evaluation_results:\n",
        "    model_name = result['model_name']\n",
        "    hyperparameters = result['hyperparameters']\n",
        "    evaluation_results = result['evaluation_results']\n",
        "    status = result.get('status')\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Hyperparameters: {hyperparameters}\")\n",
        "\n",
        "    if status == 'evaluation_failed':\n",
        "        print(f\"Status: Evaluation Failed - {result.get('error', 'No error details')}\")\n",
        "        print(\"-\" * 20)\n",
        "        continue\n",
        "    elif evaluation_results:\n",
        "        avg_similarity_score = evaluation_results['average_similarity_score']\n",
        "        print(f\"Average Similarity Score: {avg_similarity_score:.4f}\")\n",
        "\n",
        "        # Check if this model has the highest average similarity so far\n",
        "        if avg_similarity_score > highest_avg_similarity:\n",
        "            highest_avg_similarity = avg_similarity_score\n",
        "            best_model = model_name\n",
        "        print(\"-\" * 20)\n",
        "    else:\n",
        "        print(\"Evaluation results not available.\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "print(\"\\n--- Conclusion ---\")\n",
        "if best_model:\n",
        "    print(f\"Based on the average similarity score, the best performing model is:\")\n",
        "    print(f\"Model Name: {best_model}\")\n",
        "    print(f\"Highest Average Similarity Score: {highest_avg_similarity:.4f}\")\n",
        "else:\n",
        "    print(\"Could not determine the best performing model as no successful evaluation results were found.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58809c93"
      },
      "source": [
        "# Eval Functions\n",
        "Evaluate the performance of three finetuned models and at least one non-finetuned model on a set of survey question generation prompts. Generate outputs for each model, compare them to optimal outputs, and analyze the results to identify the best-performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ea40a03"
      },
      "source": [
        "## Define test prompts and optimal outputs\n",
        "\n",
        "### Subtask:\n",
        "Create a comprehensive list of test prompts and their corresponding \"optimal\" outputs in a structured format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbbf91bf"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a Python list named `test_data` containing dictionaries with 'input_prompt' and 'goal_output' keys, representing diverse survey question generation prompts and their optimal outputs as requested in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "488b309b"
      },
      "source": [
        "test_data = [\n",
        "    {\n",
        "        'input_prompt': '[q6] Please rank the below criteria in order of importance (where 1 = most important and 4 = least important) when deciding on an ADAS ECU Hardware supplier\\nClick or drag each item into a rank position.\\nRow:\\n[r1] Product technical performance (e.g., best specs and feastures, high fail safety)\\n[r2] Cost competitiveness (e.g., low sales price) 3. Quality (e.g., high reliability, little changes during development) 4. Launch (e.g., execution of planned launch schedule)',\n",
        "        'goal_output': '<select \\n  label=\"q6\"\\n  minRanks=\"3\"\\n  optional=\"1\"\\n  unique=\"none,cols\"\\n  uses=\"ranksort.7\">\\n  <title>Please rank the below criteria in order of importance (where 1 = most important and 4 = least important) when deciding on an ADAS ECU Hardware supplier\\nClick or drag each item into a rank position.</title>\\n  <comment>Click or drag each item into a rank position.</comment>\\n  <row label=\"r1\">Product technical performance (e.g., best specs and feastures, high fail safety)</row>\\n  <row label=\"r2\">Cost competitiveness (e.g., low sales price)</row>\\n  <row label=\"r3\">Quality (e.g., high reliability, little changes during development)</row>\\n  <row label=\"r4\">Launch (e.g., execution of planned launch schedule)</row>\\n  <choice label=\"ch1\">1</choice>\\n  <choice label=\"ch2\">2</choice>\\n  <choice label=\"ch3\">3</choice>\\n  <choice label=\"ch4\">4</choice>\\n</select>\\n'\n",
        "    },\n",
        "    {\n",
        "        'input_prompt': 'Please enter your company\\'s apprxoximate annual revenue in USD.',\n",
        "        'goal_output': '<number \\n  label=\"q7\"\\n  optional=\"0\"\\n  size=\"10\">\\n  <title>Please enter your company\\'s approximate annual revenue in USD.</title>\\n  <comment>Enter a number</comment>\\n</number>\\n'\n",
        "    },\n",
        "    {\n",
        "        'input_prompt': 'How satisfied are you with our customer service? (Very Satisfied, Satisfied, Neutral, Dissatisfied, Very Dissatisfied)',\n",
        "        'goal_output': '<radio \\n  label=\"q8\">\\n  <title>How satisfied are you with our customer service?</title>\\n  <comment>Select one</comment>\\n  <row label=\"r1\">Very Satisfied</row>\\n  <row label=\"r2\">Satisfied</row>\\n  <row label=\"r3\">Neutral</row>\\n  <row label=\"r4\">Dissatisfied</row>\\n  <row label=\"r5\">Very Dissatisfied</row>\\n</radio>'\n",
        "    },\n",
        "    {\n",
        "        'input_prompt': 'Please select all the fruits you like: Apple, Banana, Cherry, Date, Elderberry',\n",
        "        'goal_output': '<checkbox \\n  label=\"q9\"\\n  atleast=\"1\">\\n  <title>Please select all the fruits you like:&amp;nbsp;</title>\\n  <comment>Select all that apply</comment>\\n  <row label=\"r1\">Apple</row>\\n  <row label=\"r2\">Banana</row>\\n  <row label=\"r3\">Cherry</row>\\n  <row label=\"r4\">Date</row>\\n  <row label=\"r5\">Elderberry</row>\\n</checkbox>\\n'\n",
        "    },\n",
        "    {\n",
        "        'input_prompt': 'On a scale of 1 to 10, how likely are you to recommend our product to a friend or colleague? (1 = Not at all likely, 10 = Extremely likely)',\n",
        "        'goal_output': '<number \\n  label=\"q10\"\\n  ignoreValues=\"99\"\\n  optional=\"0\"\\n  size=\"10\"\\n  uses=\"atmrating.6\"\\n  verify=\"range(1,10)\">\\n  <title>On a scale of 1 to 10, how likely are you to recommend our product to a friend or colleague? (1 = Not at all likely, 10 = Extremely likely)</title>\\n  <comment>Click on the buttons to rate the question.</comment>\\n</number>\\n'\n",
        "    },\n",
        "    {\n",
        "        'input_prompt': 'What is your age?',\n",
        "        'goal_output': '<number \\n  label=\"q12\"\\n  optional=\"0\"\\n  size=\"10\">\\n  <title>What is your age?</title>\\n  <comment>Enter a number</comment>\\n</number>'\n",
        "    },\n",
        "    {\n",
        "        'input_prompt': 'Please provide any additional comments.',\n",
        "        'goal_output': '<textarea \\n  label=\"q13\"\\n  height=\"10\"\\n  optional=\"0\"\\n  width=\"50\">\\n  <title>Please provide any additional comments.</title>\\n  <comment>Be specific</comment>\\n</textarea>'\n",
        "    },\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive/Colab\\ Notebooks/finetuned_lora_models/unsloth_gemma_2b_bnb_4bit_1759390174/lora_finetuned_model"
      ],
      "metadata": {
        "id": "eyNb2D9NicjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e75b00f3"
      },
      "source": [
        "## Present findings\n",
        "\n",
        "### Subtask:\n",
        "Present the evaluation results in a clear and organized manner, potentially using visualizations or summary tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82fa4702"
      },
      "source": [
        "**Reasoning**:\n",
        "Display the average similarity per model and the best performing model based on the evaluation results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b5a3d4d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np # Import numpy for np.float64\n",
        "\n",
        "# Ensure the evaluation_df and average_similarity_per_model DataFrames exist\n",
        "# This assumes the previous evaluation step was successful and populated these dataframes.\n",
        "# If not, you might need to reload the data or handle the case where they are empty.\n",
        "if 'evaluation_df' not in globals() or evaluation_df.empty:\n",
        "    print(\"Evaluation results DataFrame not found or is empty. Cannot display results.\")\n",
        "else:\n",
        "    # 1. Display the average_similarity_per_model DataFrame, sorted by performance.\n",
        "    print(\"--- Average Similarity Scores per Model (Sorted) ---\")\n",
        "    display(average_similarity_per_model)\n",
        "\n",
        "    # 2. Print the name of the best performing model and its average similarity score.\n",
        "    if not average_similarity_per_model.empty:\n",
        "        # Ensure best_model_result is a Series for consistent access\n",
        "        if isinstance(best_model_result, pd.DataFrame):\n",
        "             best_model_result = best_model_result.iloc[0]\n",
        "\n",
        "        best_model_name = best_model_result['model_name']\n",
        "        highest_avg_similarity = best_model_result['similarity_score']\n",
        "        print(f\"\\n--- Best Performing Model ---\")\n",
        "        print(f\"Model Name: {best_model_name}\")\n",
        "        print(f\"Highest Average Similarity Score: {highest_avg_similarity:.4f}\")\n",
        "    else:\n",
        "        print(\"\\n--- Best Performing Model ---\")\n",
        "        print(\"Could not determine the best performing model as no evaluation results were available.\")\n",
        "\n",
        "    # 3. (Optional) Display the full evaluation_df DataFrame for detailed view.\n",
        "    # print(\"\\n--- Individual Evaluation Results ---\")\n",
        "    # display(evaluation_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "583735e2"
      },
      "source": [
        "## Analyze and Compare Results\n",
        "\n",
        "### Subtask:\n",
        "Analyze the collected evaluation metrics to compare the performance of the different models. Identify the best-performing model(s) based on the chosen metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85747125"
      },
      "source": [
        "**Reasoning**:\n",
        "The evaluation results have been collected. The next step is to analyze these results to compare the performance of different models as per the original task requirement, which involves calculating average similarity scores per model and identifying the best performer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4213773"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Convert the evaluation results list to a pandas DataFrame for easier analysis\n",
        "evaluation_df = pd.DataFrame(evaluation_results)\n",
        "\n",
        "# 2. Calculate the average similarity score for each model\n",
        "average_similarity_per_model = evaluation_df.groupby('model_name')['similarity_score'].mean().reset_index()\n",
        "average_similarity_per_model = average_similarity_per_model.sort_values(by='similarity_score', ascending=False)\n",
        "\n",
        "# 3. Print the average similarity scores per model\n",
        "print(\"\\n--- Average Similarity Scores per Model ---\")\n",
        "print(average_similarity_per_model)\n",
        "\n",
        "# 4. Identify the best performing model based on the highest average similarity score\n",
        "if not average_similarity_per_model.empty:\n",
        "    best_model_result = average_similarity_per_model.iloc[0]\n",
        "    best_model_name = best_model_result['model_name']\n",
        "    highest_avg_similarity = best_model_result['similarity_score']\n",
        "    print(f\"\\n--- Best Performing Model ---\")\n",
        "    print(f\"Model Name: {best_model_name}\")\n",
        "    print(f\"Highest Average Similarity Score: {highest_avg_similarity:.4f}\")\n",
        "else:\n",
        "    print(\"\\n--- Best Performing Model ---\")\n",
        "    print(\"Could not determine the best performing model as no evaluation results were available.\")\n",
        "\n",
        "# Optional: Display individual evaluation results\n",
        "# print(\"\\n--- Individual Evaluation Results ---\")\n",
        "# display(evaluation_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98b9d282"
      },
      "source": [
        "## Evaluate generated outputs\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the combined results and use the previously defined `evaluate_model` function (or a modified version) to compare the generated outputs to the \"optimal\" outputs. Store the evaluation metrics (e.g., similarity scores)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fe82623"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the combined model outputs, initialize the evaluation results list and the Sentence Transformer model, then iterate through the combined outputs to calculate and store the similarity scores against the goal outputs from test_data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bb1d579"
      },
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import os\n",
        "\n",
        "# 1. Load the combined model outputs from the saved JSON file\n",
        "combined_outputs_path = \"/content/drive/My Drive/Colab Notebooks/combined_model_outputs.json\"\n",
        "\n",
        "# Check if the file exists before attempting to load\n",
        "if not os.path.exists(combined_outputs_path):\n",
        "    print(f\"Error: Combined outputs file not found at {combined_outputs_path}\")\n",
        "    combined_model_outputs = [] # Initialize as empty to prevent errors\n",
        "else:\n",
        "    with open(combined_outputs_path, 'r') as f:\n",
        "        combined_model_outputs = json.load(f)\n",
        "\n",
        "# 2. Initialize an empty list to store the evaluation results\n",
        "evaluation_results = []\n",
        "\n",
        "# 3. Initialize the Sentence Transformer model for cosine similarity calculation\n",
        "try:\n",
        "    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing SentenceTransformer model: {e}\")\n",
        "    sentence_model = None # Set to None if initialization fails\n",
        "\n",
        "# Ensure test_data is available (assuming it was defined in a previous cell)\n",
        "if 'test_data' not in globals():\n",
        "    print(\"Warning: test_data not found. Cannot perform evaluation.\")\n",
        "    test_data = [] # Initialize as empty to prevent errors\n",
        "\n",
        "# 4. Iterate through each item in the loaded combined outputs list\n",
        "print(f\"\\nStarting evaluation for {len(combined_model_outputs)} combined outputs...\")\n",
        "for item in combined_model_outputs:\n",
        "    # 5. For each item, extract the input_prompt, generated_output, and the model_name\n",
        "    input_prompt = item.get('input_prompt')\n",
        "    generated_output = item.get('generated_output')\n",
        "    model_name = item.get('model_name')\n",
        "\n",
        "    if input_prompt is None or generated_output is None or model_name is None:\n",
        "        print(f\"Skipping evaluation for incomplete item: {item}\")\n",
        "        continue\n",
        "\n",
        "    # 6. Find the corresponding goal_output for the input_prompt from the test_data list\n",
        "    goal_output = None\n",
        "    for test_case in test_data:\n",
        "        if test_case.get('input_prompt') == input_prompt:\n",
        "            goal_output = test_case.get('goal_output')\n",
        "            break\n",
        "\n",
        "    if goal_output is None:\n",
        "        print(f\"Warning: No goal_output found for input prompt: {input_prompt}. Skipping evaluation for this item.\")\n",
        "        continue\n",
        "\n",
        "    # 7. Calculate the cosine similarity between the generated_output and the goal_output\n",
        "    similarity_score = 0 # Default to 0 on error\n",
        "    if sentence_model:\n",
        "        try:\n",
        "            embeddings = sentence_model.encode([generated_output, goal_output])\n",
        "            similarity_score = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating similarity for model {model_name} and prompt: {input_prompt}. Error: {e}\")\n",
        "    else:\n",
        "         print(\"SentenceTransformer model not initialized. Cannot calculate similarity.\")\n",
        "\n",
        "\n",
        "    # 8. Store the results in a dictionary\n",
        "    evaluation_results.append({\n",
        "        \"model_name\": model_name,\n",
        "        \"input_prompt\": input_prompt,\n",
        "        \"generated_output\": generated_output,\n",
        "        \"goal_output\": goal_output,\n",
        "        \"similarity_score\": similarity_score\n",
        "    })\n",
        "\n",
        "# 9. Append this dictionary to the evaluation results list (done inside the loop)\n",
        "\n",
        "# 10. After iterating through all combined outputs, print the total number of evaluation results collected\n",
        "print(f\"\\nTotal number of evaluation results collected: {len(evaluation_results)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63153ec7"
      },
      "source": [
        "## Combine and save results\n",
        "\n",
        "### Subtask:\n",
        "Combine the generated outputs from all models (finetuned and non-finetuned) into a single dataset or structure. Save this combined dataset for later analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "390d712c"
      },
      "source": [
        "**Reasoning**:\n",
        "Combine the generated outputs from finetuned and non-finetuned models, save the combined results to a JSON file, and print a confirmation message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15bcbd70"
      },
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# 1. Create a new list by combining the finetuned_model_outputs list and the non_finetuned_model_outputs list.\n",
        "combined_model_outputs = finetuned_model_outputs + non_finetuned_model_outputs\n",
        "\n",
        "# 2. Print the total number of combined outputs to verify the combination.\n",
        "print(f\"Total number of combined outputs: {len(combined_model_outputs)}\")\n",
        "\n",
        "# 3. Define a file path where you want to save the combined results.\n",
        "# Ensure the directory exists before saving\n",
        "save_directory = \"/content/drive/My Drive/Colab Notebooks\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "save_file_path = os.path.join(save_directory, \"combined_model_outputs.json\")\n",
        "\n",
        "# 4. Open the specified file path in write mode.\n",
        "# 5. Use the json.dump() function to write the combined list of outputs to the file.\n",
        "with open(save_file_path, 'w') as f:\n",
        "    json.dump(combined_model_outputs, f, indent=4)\n",
        "\n",
        "# 6. Print a confirmation message indicating that the combined results have been saved and the path to the saved file.\n",
        "print(f\"Combined model outputs saved successfully to: {save_file_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ca1f16c"
      },
      "source": [
        "### Load and generate outputs for a non-finetuned model (optional)\n",
        "\n",
        "**Subtask**:\n",
        "If desired, load a non-finetuned version of the base model and generate outputs for the same set of test prompts. Store these outputs similarly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0998ee39"
      },
      "source": [
        "**Reasoning**:\n",
        "Load a non-finetuned version of the base model and generate outputs for the same set of test prompts as specified in the instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac6b8ee6"
      },
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# 1. Define the name of the non-finetuned base model\n",
        "non_finetuned_model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\" # Or any other base model name\n",
        "\n",
        "# Use existing global variables if they exist, otherwise define defaults\n",
        "if 'max_seq_length' not in globals():\n",
        "    max_seq_length = 2048\n",
        "if 'dtype' not in globals():\n",
        "    dtype = None\n",
        "if 'load_in_4bit' not in globals():\n",
        "    load_in_4bit = True\n",
        "if 'test_data' not in globals():\n",
        "    # This should be populated from a previous step, but initialize if not found\n",
        "    test_data = []\n",
        "    print(\"Warning: test_data not found. Ensure test data definition step was executed.\")\n",
        "\n",
        "# 2. Load the non-finetuned model and its tokenizer\n",
        "print(f\"\\nLoading non-finetuned model: {non_finetuned_model_name}\")\n",
        "non_finetuned_model, non_finetuned_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = non_finetuned_model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 3. Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(non_finetuned_model)\n",
        "\n",
        "# 4. Re-apply the chat template to the tokenizer\n",
        "non_finetuned_tokenizer = get_chat_template(\n",
        "    non_finetuned_tokenizer,\n",
        "    chat_template = \"chatml\",\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
        "    map_eos_token = True,\n",
        ")\n",
        "\n",
        "# 5. Initialize an empty list to store the generated outputs\n",
        "non_finetuned_model_outputs = []\n",
        "\n",
        "print(f\"\\nGenerating outputs for non-finetuned model: {non_finetuned_model_name}\")\n",
        "\n",
        "# 6. Iterate through the test_data list\n",
        "for test_case in test_data:\n",
        "    input_prompt = test_case['input_prompt']\n",
        "\n",
        "    # 7. Prepare the input messages in the ChatML format\n",
        "    messages = [\n",
        "        {\"from\": \"human\", \"value\": input_prompt},\n",
        "        {\"from\": \"gpt\", \"value\": \"\"}, # Add an empty assistant message for generation prompt\n",
        "    ]\n",
        "\n",
        "    # 8. Apply the chat template and tokenize the messages\n",
        "    inputs = non_finetuned_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True,\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # 9. Generate the output from the non-finetuned model\n",
        "    outputs = non_finetuned_model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=256, # Set max_new_tokens\n",
        "        use_cache=True,\n",
        "        pad_token_id=non_finetuned_tokenizer.eos_token_id, # Set pad_token_id to eos_token_id\n",
        "        do_sample=True, # Enable sampling\n",
        "        top_k=50, # Sample from top 50 tokens\n",
        "        top_p=0.95, # Sample from top tokens with cumulative probability 0.95\n",
        "        temperature=0.7, # Control randomness\n",
        "    )\n",
        "\n",
        "    # 10. Decode the generated output, excluding the input prompt part\n",
        "    generated_output = non_finetuned_tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    # 11. Append a dictionary containing the model name, input_prompt, and generated_output\n",
        "    non_finetuned_model_outputs.append({\n",
        "        \"model_name\": f\"non-finetuned_{non_finetuned_model_name.replace('/', '_').replace('-', '_')}\",\n",
        "        \"input_prompt\": input_prompt,\n",
        "        \"generated_output\": generated_output\n",
        "    })\n",
        "\n",
        "# 12. After processing all test prompts, print a message\n",
        "print(f\"\\nOutput generation complete for non-finetuned model: {non_finetuned_model_name}\")\n",
        "\n",
        "# 13. Print the total number of generated outputs collected\n",
        "print(f\"Total number of generated outputs collected from non-finetuned model: {len(non_finetuned_model_outputs)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebd7f259"
      },
      "source": [
        "## Load and generate outputs for each finetuned model\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the saved finetuned LoRA models, load each one, and generate outputs for all the test prompts. Store the generated outputs along with the model name and input prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "008d6a72"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the saved finetuned LoRA models, load each one, and generate outputs for all the test prompts, storing the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6123e566"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Use existing global variables if they exist, otherwise define defaults\n",
        "if 'max_seq_length' not in globals():\n",
        "    max_seq_length = 2048\n",
        "if 'dtype' not in globals():\n",
        "    dtype = None\n",
        "if 'load_in_4bit' not in globals():\n",
        "    load_in_4bit = True\n",
        "# Ensure test_data is available (assuming it was defined in a previous cell)\n",
        "if 'test_data' not in globals() or not test_data:\n",
        "    test_data = []\n",
        "    print(\"Warning: test_data not found or is empty. Ensure test data definition step was executed with valid data.\")\n",
        "\n",
        "\n",
        "# Define the base Google Drive path where finetuned models are saved\n",
        "base_drive_path = \"/content/drive/My Drive/Colab Notebooks/finetuned_lora_models\"\n",
        "combined_outputs_path = \"/content/drive/My Drive/Colab Notebooks/combined_model_outputs.json\"\n",
        "\n",
        "# Load existing combined outputs if the file exists, otherwise initialize an empty list\n",
        "if os.path.exists(combined_outputs_path):\n",
        "    with open(combined_outputs_path, 'r') as f:\n",
        "        combined_model_outputs = json.load(f)\n",
        "else:\n",
        "    combined_model_outputs = []\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(os.path.dirname(combined_outputs_path), exist_ok=True)\n",
        "\n",
        "\n",
        "# --- Configuration for this specific run ---\n",
        "# Manually set the directory name for the model you want to process in this run\n",
        "model_directory_name = \"unsloth_mistral_7b_instruct_v0.2_bnb_4bit_epochs_2_1759585933\" # CHANGE THIS for each model\n",
        "\n",
        "model_dir = os.path.join(base_drive_path, model_directory_name)\n",
        "model_name = model_directory_name # Use directory name as model name\n",
        "\n",
        "# --- Check if this model's outputs already exist in the combined file ---\n",
        "# This prevents regenerating outputs if the step was already completed for this model\n",
        "existing_outputs_for_model = [\n",
        "    item for item in combined_model_outputs if item.get('model_name') == model_name\n",
        "]\n",
        "\n",
        "if existing_outputs_for_model and len(existing_outputs_for_model) >= len(test_data):\n",
        "    print(f\"Outputs for model '{model_name}' already found in combined outputs file. Skipping generation.\")\n",
        "else:\n",
        "    # Construct the full path to the saved LoRA adapters\n",
        "    lora_adapter_path = os.path.join(model_dir, \"lora_finetuned_model\")\n",
        "\n",
        "    # Check if the adapter path exists. If not, print a warning.\n",
        "    if not os.path.exists(lora_adapter_path):\n",
        "        print(f\"LoRA adapter directory not found at {lora_adapter_path}. Cannot generate outputs for {model_name}.\")\n",
        "    else:\n",
        "        print(f\"\\nGenerating outputs for model: {model_name}\")\n",
        "        print(f\"Loading from path: {lora_adapter_path}\")\n",
        "\n",
        "        try:\n",
        "            # Load the finetuned model and tokenizer\n",
        "            # Explicitly specify the device if necessary, although .to('cuda') below should handle it\n",
        "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "                model_name = lora_adapter_path, # Load from the saved adapter path\n",
        "                max_seq_length = max_seq_length,\n",
        "                dtype = dtype,\n",
        "                load_in_4bit = load_in_4bit,\n",
        "                device_map = \"auto\", # Use auto device map\n",
        "            )\n",
        "\n",
        "            # Ensure model is on the correct device after loading\n",
        "            # model.to(\"cuda\") # Moved to device_map=\"auto\"\n",
        "\n",
        "            # Enable native 2x faster inference\n",
        "            FastLanguageModel.for_inference(model)\n",
        "\n",
        "            # Re-apply the chat template to the tokenizer\n",
        "            tokenizer = get_chat_template(\n",
        "                tokenizer,\n",
        "                chat_template = \"chatml\",\n",
        "                mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
        "                map_eos_token = True,\n",
        "            )\n",
        "\n",
        "            # Iterate through the test_data list\n",
        "            model_outputs = [] # Temporarily store outputs for this model\n",
        "            print(f\"Generating outputs for {len(test_data)} test prompts...\")\n",
        "            for test_case in test_data:\n",
        "                input_prompt = test_case['input_prompt']\n",
        "\n",
        "                # Prepare the input messages in the ChatML format\n",
        "                messages = [\n",
        "                    {\"from\": \"human\", \"value\": input_prompt},\n",
        "                    {\"from\": \"gpt\", \"value\": \"\"}, # Add an empty assistant message for generation prompt\n",
        "                ]\n",
        "\n",
        "                # Apply the chat template and tokenize the messages\n",
        "                inputs = tokenizer.apply_chat_template(\n",
        "                    messages,\n",
        "                    tokenize = True,\n",
        "                    add_generation_prompt = True,\n",
        "                    return_tensors = \"pt\",\n",
        "                ).to(\"cuda\")\n",
        "\n",
        "                # Generate the output from the model\n",
        "                outputs = model.generate(\n",
        "                    input_ids=inputs,\n",
        "                    max_new_tokens=256, # Set max_new_tokens\n",
        "                    use_cache=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id, # Set pad_token_id to eos_token_id\n",
        "                    do_sample=True, # Enable sampling\n",
        "                    top_k=50, # Sample from top 50 tokens\n",
        "                    top_p=0.95, # Sample from top tokens with cumulative probability 0.95\n",
        "                    temperature=0.7, # Control randomness\n",
        "                )\n",
        "\n",
        "                # Decode the generated output, excluding the input prompt part\n",
        "                generated_output = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "                # Append a dictionary containing the model_name, the original input_prompt, and the generated_output\n",
        "                model_outputs.append({\n",
        "                    \"model_name\": model_name,\n",
        "                    \"input_prompt\": input_prompt,\n",
        "                    \"generated_output\": generated_output\n",
        "                })\n",
        "\n",
        "            # Append the outputs for this model to the combined list\n",
        "            combined_model_outputs.extend(model_outputs)\n",
        "\n",
        "            # Save the combined outputs list incrementally\n",
        "            with open(combined_outputs_path, 'w') as f:\n",
        "                json.dump(combined_model_outputs, f, indent=4)\n",
        "\n",
        "            print(f\"Finished generating and saving outputs for {model_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating outputs for model {model_name} from {lora_adapter_path}: {e}\")\n",
        "            print(f\"Error details: {e}\") # Print full error for debugging\n",
        "\n",
        "# 4. Print the total number of generated outputs collected so far in the combined file\n",
        "print(f\"\\nTotal number of generated outputs in combined file: {len(combined_model_outputs)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf817ced"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `unsloth` library was not found. Although there is an installation cell at the beginning of the notebook, it seems it was not executed in the current environment. I need to run the installation cell to ensure `unsloth` and other necessary libraries are available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le21oIazYTRV"
      },
      "source": [
        "%%capture\n",
        "import os, re\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.55.4\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3d4fc9a"
      },
      "source": [
        "**Reasoning**:\n",
        "The installation step has been executed. Now I can proceed with the original subtask of iterating through the saved finetuned LoRA models, loading them, generating outputs for the test prompts, and storing the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc34f08f"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# 1. Initialize an empty list called finetuned_model_outputs\n",
        "finetuned_model_outputs = []\n",
        "\n",
        "# Use existing global variables if they exist, otherwise define defaults\n",
        "if 'max_seq_length' not in globals():\n",
        "    max_seq_length = 2048\n",
        "if 'dtype' not in globals():\n",
        "    dtype = None\n",
        "if 'load_in_4bit' not in globals():\n",
        "    load_in_4bit = True\n",
        "if 'finetuning_results' not in globals():\n",
        "    # This should ideally be populated from a previous step, but initialize if not found\n",
        "    finetuning_results = []\n",
        "    print(\"Warning: finetuning_results not found. Ensure finetuning step was executed.\")\n",
        "\n",
        "if 'test_data' not in globals():\n",
        "    # This should also be populated from a previous step, but initialize if not found\n",
        "    test_data = []\n",
        "    print(\"Warning: test_data not found. Ensure test data definition step was executed.\")\n",
        "\n",
        "\n",
        "# 2. Iterate through the finetuning_results list\n",
        "for result in finetuning_results:\n",
        "    model_name = result[\"model_name\"]\n",
        "    save_path = result[\"save_path\"]\n",
        "\n",
        "    if save_path is None:\n",
        "        print(f\"Skipping output generation for failed finetuning run: {model_name}\")\n",
        "        continue\n",
        "\n",
        "    # 3. Inside the loop, for each finetuned model:\n",
        "    # a. Construct the full path to the saved LoRA adapters\n",
        "    # Assuming the save_lora_to_drive function saves to a subdirectory named \"lora_finetuned_model\"\n",
        "    lora_adapter_path = os.path.join(save_path, \"lora_finetuned_model\")\n",
        "\n",
        "    # b. Check if the adapter path exists. If not, print a warning and skip this model.\n",
        "    if not os.path.exists(lora_adapter_path):\n",
        "        print(f\"LoRA adapter directory not found at {lora_adapter_path}. Skipping output generation for {model_name}.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nGenerating outputs for model: {model_name}\")\n",
        "    print(f\"Loading from path: {lora_adapter_path}\")\n",
        "\n",
        "    try:\n",
        "        # c. Load the finetuned model and tokenizer\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name = lora_adapter_path, # Load from the saved adapter path\n",
        "            max_seq_length = max_seq_length,\n",
        "            dtype = dtype,\n",
        "            load_in_4bit = load_in_4bit,\n",
        "        )\n",
        "\n",
        "        # d. Enable native 2x faster inference\n",
        "        FastLanguageModel.for_inference(model)\n",
        "\n",
        "        # e. Re-apply the chat template to the tokenizer\n",
        "        tokenizer = get_chat_template(\n",
        "            tokenizer,\n",
        "            chat_template = \"chatml\",\n",
        "            mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
        "            map_eos_token = True,\n",
        "        )\n",
        "\n",
        "        # f. Iterate through the test_data list\n",
        "        for test_case in test_data:\n",
        "            input_prompt = test_case['input_prompt']\n",
        "\n",
        "            # g. For each test prompt in test_data:\n",
        "            # i. Prepare the input messages in the ChatML format\n",
        "            messages = [\n",
        "                {\"from\": \"human\", \"value\": input_prompt},\n",
        "                {\"from\": \"gpt\", \"value\": \"\"}, # Add an empty assistant message for generation prompt\n",
        "            ]\n",
        "\n",
        "            # ii. Apply the chat template and tokenize the messages\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize = True,\n",
        "                add_generation_prompt = True,\n",
        "                return_tensors = \"pt\",\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "            # iii. Generate the output from the model\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs,\n",
        "                max_new_tokens=256, # Set max_new_tokens\n",
        "                use_cache=True,\n",
        "                pad_token_id=tokenizer.eos_token_id, # Set pad_token_id to eos_token_id\n",
        "                do_sample=True, # Enable sampling\n",
        "                top_k=50, # Sample from top 50 tokens\n",
        "                top_p=0.95, # Sample from top tokens with cumulative probability 0.95\n",
        "                temperature=0.7, # Control randomness\n",
        "            )\n",
        "\n",
        "            # iv. Decode the generated output, excluding the input prompt part\n",
        "            generated_output = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "            # v. Append a dictionary containing the model_name, the original input_prompt, and the generated_output\n",
        "            finetuned_model_outputs.append({\n",
        "                \"model_name\": model_name,\n",
        "                \"input_prompt\": input_prompt,\n",
        "                \"generated_output\": generated_output\n",
        "            })\n",
        "\n",
        "        # h. After processing all test prompts for a model, print a message\n",
        "        print(f\"Finished generating outputs for {model_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating outputs for model {model_name} from {lora_adapter_path}: {e}\")\n",
        "\n",
        "# 4. After iterating through all finetuned models, print the total number of generated outputs collected\n",
        "print(f\"\\nTotal number of generated outputs collected from finetuned models: {len(finetuned_model_outputs)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3703bb3b"
      },
      "source": [
        "## Load and generate outputs for a non-finetuned model (optional)\n",
        "\n",
        "### Subtask:\n",
        "If desired, load a non-finetuned version of the base model and generate outputs for the same set of test prompts. Store these outputs similarly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9de8c1b"
      },
      "source": [
        "**Reasoning**:\n",
        "Load a non-finetuned version of the base model and generate outputs for the same set of test prompts as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b06924e"
      },
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from transformers import TextStreamer\n",
        "import os\n",
        "import json # Import json for saving\n",
        "\n",
        "# 1. Define the name of the non-finetuned base model\n",
        "non_finetuned_model_name = \"unsloth/gemma-7b-bnb-4bit\" # Or any other base model name - CHANGE IF NEEDED\n",
        "\n",
        "# Use existing global variables if they exist, otherwise define defaults\n",
        "if 'max_seq_length' not in globals():\n",
        "    max_seq_length = 2048\n",
        "if 'dtype' not in globals():\n",
        "    dtype = None\n",
        "if 'load_in_4bit' not in globals():\n",
        "    load_in_4bit = True\n",
        "if 'test_data' not in globals():\n",
        "    # This should be populated from a previous step, but initialize if not found\n",
        "    test_data = []\n",
        "    print(\"Warning: test_data not found. Ensure test data definition step was executed.\")\n",
        "\n",
        "# Define the path for the combined outputs file\n",
        "combined_outputs_path = \"/content/drive/My Drive/Colab Notebooks/combined_model_outputs.json\"\n",
        "\n",
        "# Load existing combined outputs if the file exists, otherwise initialize an empty list\n",
        "if os.path.exists(combined_outputs_path):\n",
        "    with open(combined_outputs_path, 'r') as f:\n",
        "        combined_model_outputs = json.load(f)\n",
        "else:\n",
        "    combined_model_outputs = []\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(os.path.dirname(combined_outputs_path), exist_ok=True)\n",
        "\n",
        "\n",
        "# 2. Load the non-finetuned model and its tokenizer\n",
        "print(f\"\\nLoading non-finetuned model: {non_finetuned_model_name}\")\n",
        "non_finetuned_model, non_finetuned_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = non_finetuned_model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 3. Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(non_finetuned_model)\n",
        "\n",
        "# 4. Re-apply the chat template to the tokenizer\n",
        "non_finetuned_tokenizer = get_chat_template(\n",
        "    non_finetuned_tokenizer,\n",
        "    chat_template = \"chatml\",\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
        "    map_eos_token = True,\n",
        ")\n",
        "\n",
        "# 5. Initialize a temporary list to store outputs for this run\n",
        "non_finetuned_model_run_outputs = []\n",
        "\n",
        "print(f\"\\nGenerating outputs for non-finetuned model: {non_finetuned_model_name}\")\n",
        "\n",
        "# 6. Iterate through the test_data list\n",
        "for test_case in test_data:\n",
        "    input_prompt = test_case['input_prompt']\n",
        "    # Add the specified sentence to the beginning of the input prompt\n",
        "    modified_input_prompt = \"Convert the following question into a Decipher / Forsta Surveys XML script:\" + input_prompt\n",
        "\n",
        "\n",
        "    # 7. Prepare the input messages in the ChatML format\n",
        "    messages = [\n",
        "        {\"from\": \"human\", \"value\": modified_input_prompt}, # Use the modified prompt here\n",
        "        {\"from\": \"gpt\", \"value\": \"\"}, # Add an empty assistant message for generation prompt\n",
        "    ]\n",
        "\n",
        "    # 8. Apply the chat template and tokenize the messages\n",
        "    inputs = non_finetuned_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True,\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # 9. Generate the output from the non-finetuned model\n",
        "    outputs = non_finetuned_model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=256, # Set max_new_tokens\n",
        "        use_cache=True,\n",
        "        pad_token_id=non_finetuned_tokenizer.eos_token_id, # Set pad_token_id to eos_token_id\n",
        "        do_sample=True, # Enable sampling\n",
        "        top_k=50, # Sample from top 50 tokens\n",
        "        top_p=0.95, # Sample from top tokens with cumulative probability 0.95\n",
        "        temperature=0.7, # Control randomness\n",
        "    )\n",
        "\n",
        "    # 10. Decode the generated output, *excluding* the input prompt part\n",
        "    generated_output = non_finetuned_tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    # 11. Append a dictionary containing the model name, input_prompt, and generated_output to the temporary list\n",
        "    model_name_tag = f\"non-finetuned_{non_finetuned_model_name.replace('/', '_').replace('-', '_')}\"\n",
        "    # Remove previous outputs for this specific model from the combined list before adding new ones\n",
        "    global combined_model_outputs\n",
        "    combined_model_outputs = [\n",
        "        item for item in combined_model_outputs if item.get('model_name') != model_name_tag\n",
        "    ]\n",
        "    non_finetuned_model_run_outputs.append({\n",
        "        \"model_name\": model_name_tag,\n",
        "        \"input_prompt\": input_prompt, # Store the original input prompt\n",
        "        \"generated_output\": generated_output\n",
        "    })\n",
        "\n",
        "# 12. Extend the combined outputs list with the outputs from this run\n",
        "combined_model_outputs.extend(non_finetuned_model_run_outputs)\n",
        "\n",
        "# 13. Save the combined outputs list back to the JSON file\n",
        "with open(combined_outputs_path, 'w') as f:\n",
        "    json.dump(combined_model_outputs, f, indent=4)\n",
        "\n",
        "\n",
        "# 14. After processing all test prompts, print a message\n",
        "print(f\"\\nOutput generation complete for non-finetuned model: {non_finetuned_model_name}\")\n",
        "\n",
        "# 15. Print the total number of generated outputs collected so far in the combined file\n",
        "print(f\"Total number of generated outputs collected in combined file: {len(combined_model_outputs)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71b8098a"
      },
      "source": [
        "## Combine and save results\n",
        "\n",
        "### Subtask:\n",
        "Combine the generated outputs from all models (finetuned and non-finetuned) into a single dataset or structure. Save this combined dataset for later analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5fc96e8"
      },
      "source": [
        "**Reasoning**:\n",
        "Combine the generated outputs from finetuned and non-finetuned models, save the combined results to a JSON file, and print a confirmation message.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05bf315e"
      },
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# 1. Create a new list by combining the finetuned_model_outputs list and the non_finetuned_model_outputs list.\n",
        "combined_model_outputs = finetuned_model_outputs + non_finetuned_model_outputs\n",
        "\n",
        "# 2. Print the total number of combined outputs to verify the combination.\n",
        "print(f\"Total number of combined outputs: {len(combined_model_outputs)}\")\n",
        "\n",
        "# 3. Define a file path where you want to save the combined results.\n",
        "# Ensure the directory exists before saving\n",
        "save_directory = \"/content/drive/My Drive/Colab Notebooks\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "save_file_path = os.path.join(save_directory, \"combined_model_outputs.json\")\n",
        "\n",
        "# 4. Open the specified file path in write mode.\n",
        "# 5. Use the json.dump() function to write the combined list of outputs to the file.\n",
        "with open(save_file_path, 'w') as f:\n",
        "    json.dump(combined_model_outputs, f, indent=4)\n",
        "\n",
        "# 6. Print a confirmation message indicating that the combined results have been saved and the path to the saved file.\n",
        "print(f\"Combined model outputs saved successfully to: {save_file_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "173066a7"
      },
      "source": [
        "## Evaluate generated outputs\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the combined results and use the previously defined `evaluate_model` function (or a modified version) to compare the generated outputs to the \"optimal\" outputs. Store the evaluation metrics (e.g., similarity scores).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c27b9810"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the combined model outputs, initialize the evaluation results list and the Sentence Transformer model, then iterate through the combined outputs to calculate and store the similarity scores against the goal outputs from test_data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d653b998"
      },
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import os\n",
        "\n",
        "# 1. Load the combined model outputs from the saved JSON file\n",
        "combined_outputs_path = \"/content/drive/My Drive/Colab Notebooks/combined_model_outputs.json\"\n",
        "\n",
        "# Check if the file exists before attempting to load\n",
        "if not os.path.exists(combined_outputs_path):\n",
        "    print(f\"Error: Combined outputs file not found at {combined_outputs_path}\")\n",
        "    combined_model_outputs = [] # Initialize as empty to prevent errors\n",
        "else:\n",
        "    with open(combined_outputs_path, 'r') as f:\n",
        "        combined_model_outputs = json.load(f)\n",
        "\n",
        "# 2. Initialize an empty list to store the evaluation results\n",
        "evaluation_results = []\n",
        "\n",
        "# 3. Initialize the Sentence Transformer model for cosine similarity calculation\n",
        "try:\n",
        "    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing SentenceTransformer model: {e}\")\n",
        "    sentence_model = None # Set to None if initialization fails\n",
        "\n",
        "# Ensure test_data is available (assuming it was defined in a previous cell)\n",
        "if 'test_data' not in globals():\n",
        "    print(\"Warning: test_data not found. Cannot perform evaluation.\")\n",
        "    test_data = [] # Initialize as empty to prevent errors\n",
        "\n",
        "# 4. Iterate through each item in the loaded combined outputs list\n",
        "print(f\"\\nStarting evaluation for {len(combined_model_outputs)} combined outputs...\")\n",
        "for item in combined_model_outputs:\n",
        "    # 5. For each item, extract the input_prompt, generated_output, and the model_name\n",
        "    input_prompt = item.get('input_prompt')\n",
        "    generated_output = item.get('generated_output')\n",
        "    model_name = item.get('model_name')\n",
        "\n",
        "    if input_prompt is None or generated_output is None or model_name is None:\n",
        "        print(f\"Skipping evaluation for incomplete item: {item}\")\n",
        "        continue\n",
        "\n",
        "    # 6. Find the corresponding goal_output for the input_prompt from the test_data list\n",
        "    goal_output = None\n",
        "    for test_case in test_data:\n",
        "        if test_case.get('input_prompt') == input_prompt:\n",
        "            goal_output = test_case.get('goal_output')\n",
        "            break\n",
        "\n",
        "    if goal_output is None:\n",
        "        print(f\"Warning: No goal_output found for input prompt: {input_prompt}. Skipping evaluation for this item.\")\n",
        "        continue\n",
        "\n",
        "    # 7. Calculate the cosine similarity between the generated_output and the goal_output\n",
        "    similarity_score = 0 # Default to 0 on error\n",
        "    if sentence_model:\n",
        "        try:\n",
        "            embeddings = sentence_model.encode([generated_output, goal_output])\n",
        "            similarity_score = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating similarity for model {model_name} and prompt: {input_prompt}. Error: {e}\")\n",
        "    else:\n",
        "         print(\"SentenceTransformer model not initialized. Cannot calculate similarity.\")\n",
        "\n",
        "\n",
        "    # 8. Store the results in a dictionary\n",
        "    evaluation_results.append({\n",
        "        \"model_name\": model_name,\n",
        "        \"input_prompt\": input_prompt,\n",
        "        \"generated_output\": generated_output,\n",
        "        \"goal_output\": goal_output,\n",
        "        \"similarity_score\": similarity_score\n",
        "    })\n",
        "\n",
        "# 9. Append this dictionary to the evaluation results list (done inside the loop)\n",
        "\n",
        "# 10. After iterating through all combined outputs, print the total number of evaluation results collected\n",
        "print(f\"\\nTotal number of evaluation results collected: {len(evaluation_results)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f321eb1"
      },
      "source": [
        "**Reasoning**:\n",
        "The evaluation results have been collected. The next step is to analyze these results to compare the performance of different models as per the original task requirement, which involves calculating average similarity scores per model and identifying the best performer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56bea0f7"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Convert the evaluation results list to a pandas DataFrame for easier analysis\n",
        "evaluation_df = pd.DataFrame(evaluation_results)\n",
        "\n",
        "# 2. Calculate the average similarity score for each model\n",
        "average_similarity_per_model = evaluation_df.groupby('model_name')['similarity_score'].mean().reset_index()\n",
        "average_similarity_per_model = average_similarity_per_model.sort_values(by='similarity_score', ascending=False)\n",
        "\n",
        "# 3. Print the average similarity scores per model\n",
        "print(\"\\n--- Average Similarity Scores per Model ---\")\n",
        "print(average_similarity_per_model)\n",
        "\n",
        "# 4. Identify the best performing model based on the highest average similarity score\n",
        "if not average_similarity_per_model.empty:\n",
        "    best_model_result = average_similarity_per_model.iloc[0]\n",
        "    best_model_name = best_model_result['model_name']\n",
        "    highest_avg_similarity = best_model_result['similarity_score']\n",
        "    print(f\"\\n--- Best Performing Model ---\")\n",
        "    print(f\"Model Name: {best_model_name}\")\n",
        "    print(f\"Highest Average Similarity Score: {highest_avg_similarity:.4f}\")\n",
        "else:\n",
        "    print(\"\\n--- Best Performing Model ---\")\n",
        "    print(\"Could not determine the best performing model as no evaluation results were available.\")\n",
        "\n",
        "# Optional: Display individual evaluation results\n",
        "# print(\"\\n--- Individual Evaluation Results ---\")\n",
        "# display(evaluation_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7934bc27"
      },
      "source": [
        "## Present findings\n",
        "\n",
        "### Subtask:\n",
        "Present the evaluation results in a clear and organized manner, potentially using visualizations or summary tables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05dd9670"
      },
      "source": [
        "**Reasoning**:\n",
        "Display the average similarity per model and the best performing model based on the evaluation results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "add9a9ab"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np # Import numpy for np.float64\n",
        "\n",
        "# Ensure the evaluation_df and average_similarity_per_model DataFrames exist\n",
        "# This assumes the previous evaluation step was successful and populated these dataframes.\n",
        "# If not, you might need to reload the data or handle the case where they are empty.\n",
        "if 'evaluation_df' not in globals() or evaluation_df.empty:\n",
        "    print(\"Evaluation results DataFrame not found or is empty. Cannot display results.\")\n",
        "else:\n",
        "    # 1. Display the average_similarity_per_model DataFrame, sorted by performance.\n",
        "    print(\"--- Average Similarity Scores per Model (Sorted) ---\")\n",
        "    display(average_similarity_per_model)\n",
        "\n",
        "    # 2. Print the name of the best performing model and its average similarity score.\n",
        "    if not average_similarity_per_model.empty:\n",
        "        # Ensure best_model_result is a Series for consistent access\n",
        "        if isinstance(best_model_result, pd.DataFrame):\n",
        "             best_model_result = best_model_result.iloc[0]\n",
        "\n",
        "        best_model_name = best_model_result['model_name']\n",
        "        highest_avg_similarity = best_model_result['similarity_score']\n",
        "        print(f\"\\n--- Best Performing Model ---\")\n",
        "        print(f\"Model Name: {best_model_name}\")\n",
        "        print(f\"Highest Average Similarity Score: {highest_avg_similarity:.4f}\")\n",
        "    else:\n",
        "        print(\"\\n--- Best Performing Model ---\")\n",
        "        print(\"Could not determine the best performing model as no evaluation results were available.\")\n",
        "\n",
        "    # 3. (Optional) Display the full evaluation_df DataFrame for detailed view.\n",
        "    # print(\"\\n--- Individual Evaluation Results ---\")\n",
        "    # display(evaluation_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "37d6d46e",
        "vITh0KVJ10qX",
        "idAEIeSQ3xdS",
        "ekOmTR1hSNcr",
        "f422JgM9sdVT",
        "TCv4vXHd61i7"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}